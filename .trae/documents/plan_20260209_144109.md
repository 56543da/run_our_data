# 解决 Loss 异常与添加 AMP 控制开关计划

## 1. 修复 Loss/Sensitivity/Specificity 显示异常
问题根源：`model_evaluator1.py` 中捕获到 `ValueError`（可能是由于 NaN 输出或除以零）时，将 Loss 硬编码为 `99.0`，其他指标为 `0.0`。此外，Sensitivity/Specificity 计算时虽然做了分母非零检查，但如果在异常处理块外，可能会因为预测全为同一类导致指标计算库返回未定义值（通常是 0 或 NaN）。截图中的 `-1` 可能是前端展示时的占位符，或者代码逻辑中某些未看到的路径返回了 `-1`。
我们看到的 `Loss=99.0` 是 `model_evaluator1.py:733` 的硬编码行为。

**修改方案：**
- 修改 `evaluator/model_evaluator1.py` 中的异常处理逻辑，记录更详细的错误信息。
- 确保 `sensitivity` 和 `specificity` 计算即使在混淆矩阵某些值为 0 时也能返回合理值（0.0 而非报错）。
- 将硬编码的 `99.0` 和 `0.0` 调整为更明确的标识，或者仅打印警告而不破坏 TensorBoard 曲线（可选）。目前为了稳定性，至少要确保除法安全。

## 2. 添加 AMP (自动混合精度) 控制开关
目前 `train1.py` 中 AMP 似乎是根据 `torch.cuda.is_available()` 自动开启的，没有通过参数控制。

**修改方案：**
1.  **修改 `args/train_arg_parser.py`**:
    - 添加 `--use_amp` 参数，类型为布尔值，默认 `True`（保持现有行为，但可关闭）。
    - `self.parser.add_argument('--use_amp', type=util.str_to_bool, default=True, help='Enable Automatic Mixed Precision (AMP)')`

2.  **修改 `train1.py`**:
    - 使用 `args.use_amp` 来控制是否初始化 `GradScaler` 和启用 `autocast`。
    - 将硬编码的 `use_amp = torch.cuda.is_available()` 修改为 `use_amp = args.use_amp and torch.cuda.is_available()`。

3.  **修改 `run_multi_experiments.ps1`**:
    - 在实验配置数组 `$experiments` 中添加 `amp = $true` 或 `$false` 的字段。
    - 在生成 `$trainArgs` 时，添加 `--use_amp=$($exp.amp)`。
    - 为现有实验配置添加默认值（例如 `$true`）。

## 执行步骤
1.  **修改 `args/train_arg_parser.py`**：加入 `--use_amp` 参数。
2.  **修改 `train1.py`**：集成 `args.use_amp` 控制逻辑。
3.  **修改 `evaluator/model_evaluator1.py`**：增强指标计算的鲁棒性，防止除零和 NaN 导致的崩溃。
4.  **修改 `run_multi_experiments.ps1`**：暴露 AMP 开关给用户。
